{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "09921334",
   "metadata": {},
   "source": [
    "## Capstone Project Title: Developmet of a Natural Language Multi-lingual Chatbot for the Kenyan Market"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6e83dbe",
   "metadata": {},
   "source": [
    "## Team Members \n",
    "\n",
    "1. Jessica Mutiso\n",
    "2. Brian Waweru\n",
    "3. Pamela Godia\n",
    "4. Hellen Mwaniki"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af1b0925",
   "metadata": {},
   "source": [
    "## 1. Project Overview \n",
    "\n",
    "This project aims to develop a natural language chatbot capable of generating human-like responses and understanding informal customer feedback expressed in English, Kenyan Swahili and Sheng. Designed for a startup expanding into the Kenyan market, the chatbot will help the company engage users more naturally and analyze feedback from social platforms and online conversations. By training on locally relevant dialogue data  including YouTube comments and Kenyan media the system will capture the linguistic and cultural nuances often missed by standard models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ff3cf3f",
   "metadata": {},
   "source": [
    "## 1.1 Problem Statement\n",
    "Startups entering new markets often struggle to understand customer feedback when it's expressed in local dialects or informal language. In Kenya, much of this communication occurs in Swahili and Sheng, which combine local slang, English, and Swahili in a fluid, often unstructured manner. Existing chatbot systems trained on formal English fail to grasp the tone, intent, or meaning behind such messages. This project aims to fill that gap by building a chatbot trained specifically on real-world Kenyan conversations to interpret and respond to customer queries and feedback with local context and relevance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96b56c85",
   "metadata": {},
   "source": [
    "## 1.2 Objectives\n",
    "\n",
    "- Collect and preprocess Kenyan user dialogue from YouTube, social media, and local content featuring Swahili and Sheng\n",
    "\n",
    "- Fine-tune the chatbot with foundational data for conversational structure, while emphasizing local language patterns\n",
    "\n",
    "- Build a sequence-to-sequence model  capable of handling informal, code-switched dialogue\n",
    "\n",
    "- Evaluate the chatbot‚Äôs performance with emphasis on contextual relevance and local understanding\n",
    "\n",
    "- Present a working prototype that simulates real customer feedback scenarios "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "753a36ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, os, csv\n",
    "import pandas as pd \n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.neighbors import NearestNeighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cab5dda5",
   "metadata": {},
   "source": [
    "# 2.0 Data Scraping and understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eebbcfb1",
   "metadata": {},
   "source": [
    "The Cornell Movie Dialogues Corpus contains fictional conversations from movie scripts. It is made up of several text files, but the two we are using here are:\n",
    "\n",
    "- `movie_lines.txt` ‚Äì contains individual lines of dialogue.\n",
    "\n",
    "- `movie_conversations.txt` ‚Äì contains sequences of line IDs, showing how those lines form a conversation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "14bd3606",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Conversation 1\n",
      "Can we make this quick?  Roxanne Korrine and Andrew Barrett are having an incredibly horrendous public break- up on the quad.  Again.\n",
      "Well, I thought we'd start with pronunciation, if that's okay with you.\n",
      "Not the hacking and gagging and spitting part.  Please.\n",
      "Okay... then how 'bout we try out some French cuisine.  Saturday?  Night?\n",
      "\n",
      "Conversation 2\n",
      "You're asking me out.  That's so cute. What's your name again?\n",
      "Forget it.\n",
      "\n",
      "Conversation 3\n",
      "No, no, it's my fault -- we didn't have a proper introduction ---\n",
      "Cameron.\n",
      "The thing is, Cameron -- I'm at the mercy of a particularly hideous breed of loser.  My sister.  I can't date until she does.\n",
      "Seems like she could get a date easy enough...\n"
     ]
    }
   ],
   "source": [
    "# load the cornell `movie_lines.txt` dataset\n",
    "# step 1 - Loading and Parsing movie_lines.txt\n",
    "# GOAL: Create a dict `id2line` that maps a `line ID` to its actual dialogue.\n",
    "movie_lines_path = r'original-data\\movie_lines.txt'\n",
    "# the dictionary\n",
    "id2line = {}\n",
    "# parsing\n",
    "with open(movie_lines_path, encoding='utf-8', errors='ignore') as f:\n",
    "    for line in f:\n",
    "        parts = line.strip().split(\" +++$+++ \")\n",
    "        if len(parts) == 5:\n",
    "            line_id, _, _, _, text = parts\n",
    "            id2line[line_id] = text\n",
    "# step 2 - Load and Parse `movie_conversations.txt`\n",
    "# GOAL: To create a list of conversations, where each conversation \n",
    "# is a list of line IDs i.e. Extract conversations - list of line IDs\n",
    "movie_conversations_path = r\"original-data\\movie_conversations.txt\"\n",
    "# list of conversations\n",
    "conversations = []\n",
    "# parsing\n",
    "with open(movie_conversations_path, encoding='utf-8', errors='ignore') as f:\n",
    "    for line in f:\n",
    "        parts = line.strip().split(\" +++$+++ \")\n",
    "        if len(parts) == 4:\n",
    "            line_ids_str = parts[3]\n",
    "            # Convert string to actual list of line IDs\n",
    "            line_ids = eval(line_ids_str)  # Safe here because the format is consistent\n",
    "            conversations.append(line_ids)\n",
    "# step 3 - Reconstruct Conversations from Line IDs\n",
    "# GOAL: Convert line IDs back into actual text using the \n",
    "# `id2line` dictionary.\n",
    "# dictionary container variable\n",
    "reconstructed_conversations = []\n",
    "#  parsing\n",
    "for conv in conversations:\n",
    "    dialogue = []\n",
    "    for line_id in conv:\n",
    "        line_text = id2line.get(line_id, \"\")\n",
    "        dialogue.append(line_text)\n",
    "    reconstructed_conversations.append(dialogue)\n",
    "# step 4 - View Sample Conversations\n",
    "# first 3 conversations\n",
    "for i, convo in enumerate(reconstructed_conversations[:3]):\n",
    "    print(f\"\\nConversation {i+1}\")\n",
    "    for turn in convo:\n",
    "        print(turn)\n",
    "# Save to Text File\n",
    "with open(\"reconstructed_conversations.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for convo in reconstructed_conversations:\n",
    "        for turn in convo:\n",
    "            f.write(turn + \"\\n\")\n",
    "        f.write(\"\\n\" + \"-\"*40 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05dee0bd",
   "metadata": {},
   "source": [
    "### 2.1 Saving the `reconstructed_conversations` file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "49cf6196",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Yes, file successfully saved!\n"
     ]
    }
   ],
   "source": [
    "# Target folder \n",
    "output_folder = \"chatbot-repo\"\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Full path to save the file\n",
    "output_file_path = os.path.join(output_folder, \"reconstructed_conversations.txt\")\n",
    "\n",
    "# Save the reconstructed conversations into the file\n",
    "with open(output_file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    for convo in reconstructed_conversations:\n",
    "        for turn in convo:\n",
    "            f.write(turn + \"\\n\")\n",
    "        f.write(\"\\n\" + \"-\"*40 + \"\\n\")\n",
    "\n",
    "# confirmation\n",
    "## print(f\"File saved to: {os.path.abspath(output_file_path)}\")\n",
    "# Confirm that the file exists\n",
    "if os.path.exists(output_file_path):\n",
    "    print(\"‚úÖ Yes, file successfully saved!\")\n",
    "    # print(\"üìÅ Location:\", os.path.abspath(output_file_path))\n",
    "else:\n",
    "    print(\"‚ùå Failed to save the file.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f85f094",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "- `id2line` has mapped line IDs to actual dialogue text.\n",
    "\n",
    "- `conversations` then holds the sequences of line IDs per conversation.\n",
    "\n",
    "- `reconstructed_conversations` gives you the full text of the conversations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57550238",
   "metadata": {},
   "source": [
    "### 2.2 YouTube datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pamela's file has the shape: (25629, 2)\n",
      "Brian's file has the shape: (2853, 2)\n",
      "\n",
      "\n",
      "Pamela's YouTube File\n",
      "      video_id                                            comment\n",
      "0  qlZM3McwO1Q  What an incredible victory. I agree the Kenyan...\n",
      "1  qlZM3McwO1Q                                                  ‚ù§\n",
      "2  qlZM3McwO1Q  ‚ÄúClaudia is an amazonian goddess with a beauti...\n",
      "3  qlZM3McwO1Q  Proud of my motherland Kenya ‚ù§‚ù§‚ù§and Africa.at ...\n",
      "4  qlZM3McwO1Q                                               Damn\n",
      "\n",
      "\n",
      "Brian's YouTube File\n",
      "                                         Top Comment Reply\n",
      "0  Apple missed the boat on AI OR... Apple is doi...   NaN\n",
      "1  Who added the background music to the video it...   NaN\n",
      "2                    16:26  FEMI KUTI !!! RAAHHH !!!   NaN\n",
      "3            The greatest AI scam in history, is AI.   NaN\n",
      "4  if I only knew Siri is a mess, I would bought ...   NaN\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "yt_file_path_pamela = r'original-data\\pamela-youtube_comments.csv'\n",
    "yt_file_path_brian = r'original-data\\brian_youtube_data_comments.csv'\n",
    "\n",
    "# Pamela's file\n",
    "yt_1 = pd.read_csv(yt_file_path_pamela)\n",
    "print(\"Pamela's file has the shape:\", yt_1.shape)\n",
    "\n",
    "# Brian's file\n",
    "yt_2 = pd.read_csv(yt_file_path_brian)\n",
    "print(\"Brian's file has the shape:\", yt_2.shape)\n",
    "print('\\n')\n",
    "\n",
    "print(\"Pamela's YouTube File\")\n",
    "print(yt_1.head(), end = '\\n\\n\\n')\n",
    "\n",
    "print(\"Brian's YouTube File\")\n",
    "print(yt_2.head(), end = '\\n\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3bcdc0f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pamela's YouTube File\n",
      "Now, Pamela's file has the shape: (25629, 1)\n",
      "                                             comment\n",
      "0  What an incredible victory. I agree the Kenyan...\n",
      "1                                                  ‚ù§\n",
      "2  ‚ÄúClaudia is an amazonian goddess with a beauti...\n",
      "3  Proud of my motherland Kenya ‚ù§‚ù§‚ù§and Africa.at ...\n",
      "4                                               Damn\n",
      "\n",
      "\n",
      "Brian's YouTube File\n",
      "Now, Brian's file has the shape: (2853, 1)\n",
      "                                         Top Comment\n",
      "0  Apple missed the boat on AI OR... Apple is doi...\n",
      "1  Who added the background music to the video it...\n",
      "2                    16:26  FEMI KUTI !!! RAAHHH !!!\n",
      "3            The greatest AI scam in history, is AI.\n",
      "4  if I only knew Siri is a mess, I would bought ...\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Cleaning yt_1 file\n",
    "## i.e dropping first column\n",
    "yt_1 = yt_1.drop(yt_1.columns[0], axis=1)\n",
    "\n",
    "# Cleaning yt_2 file\n",
    "yt_2 = yt_2.drop(yt_2.columns[1], axis=1)\n",
    "\n",
    "print(\"Pamela's YouTube File\")\n",
    "print(\"Now, Pamela's file has the shape:\", yt_1.shape)\n",
    "print(yt_1.head(), end = '\\n\\n\\n')\n",
    "\n",
    "print(\"Brian's YouTube File\")\n",
    "print(\"Now, Brian's file has the shape:\", yt_2.shape)\n",
    "print(yt_2.head(), end = '\\n\\n\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2356d19b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Shape of the combine YouTube data is: (28482, 2)\n"
     ]
    }
   ],
   "source": [
    "# combineing the two together\n",
    "yt_combined = pd.concat([yt_1, yt_2], axis=0, ignore_index=True)\n",
    "# Thus...\n",
    "print(\"The Shape of the combine YouTube data is:\", yt_combined.shape)\n",
    "\n",
    "# comfirming succesful stacking\n",
    "# print(yt_1.shape[0] + yt_2.shape[0] == yt_combined.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "238c0643",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success, `yt_combined` has been saved to chatbot-repo.\n"
     ]
    }
   ],
   "source": [
    "# Folder and file path parameters\n",
    "folder_path = 'chatbot-repo'\n",
    "file_path = os.path.join(folder_path, 'yt_combined.txt')\n",
    "\n",
    "# Save the DataFrame to a txt file\n",
    "yt_combined.to_csv(file_path, sep='\\t', index=False)\n",
    "\n",
    "# Confirmation message\n",
    "print(f\"Success, `yt_combined` has been saved to {folder_path}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fe5831b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files combined and saved to chatbot-repo\\nlp_data.txt\n"
     ]
    }
   ],
   "source": [
    "# File names\n",
    "file1 = r'chatbot-repo\\reconstructed_conversations.txt' # Cornell-uni\n",
    "file2 = r'chatbot-repo\\yt_combined.txt' # youtube-comments\n",
    "nlp_data = r'chatbot-repo\\nlp_data.txt'  # Output file is literally named \"nlp_data.txt\"\n",
    "\n",
    "# Read both files\n",
    "with open(file1, \"r\", encoding=\"utf-8\") as f1, open(file2, \"r\", encoding=\"utf-8\") as f2:\n",
    "    content1 = f1.read()\n",
    "    content2 = f2.read()\n",
    "\n",
    "# Combine contents\n",
    "combined_data = content1 + \"\\n\" + content2\n",
    "\n",
    "# Write to output file\n",
    "with open(nlp_data, \"w\", encoding=\"utf-8\") as out:\n",
    "    out.write(combined_data)\n",
    "\n",
    "print(f\"Files combined and saved to {nlp_data}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fe2b243",
   "metadata": {},
   "source": [
    "# 3.0 Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "55c405d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing\n",
    "# Function to clean the data\n",
    "def clean_text(text):\n",
    "\n",
    "    if pd.isnull(text):\n",
    "        return \"\"\n",
    "    \n",
    "    text = text.lower()                          # Lowercase all text\n",
    "    text = re.sub(r\"http\\S+|www\\S+\", \"\", text)   # Remove URLs\n",
    "    text = re.sub(r\"@\\w+\", \"\", text)             # Remove mentions\n",
    "    text = re.sub(r\"#\\w+\", \"\", text)             # Remove hashtags\n",
    "    text = re.sub(r\"[^\\w\\s]\", \"\", text)          # Remove punctuation\n",
    "    text = re.sub(r\"\\d+\", \"\", text)              # Remove digits\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()     # Remove extra whitespace\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "76c66589",
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "\n",
    "def clean_text_file(input_file, output_file=\"cleaned_output.txt\"):\n",
    "    # Read the input file\n",
    "    with open(input_file, 'r', encoding='utf-8') as f:\n",
    "        original_text = f.read()\n",
    "    \n",
    "    # Clean the text\n",
    "    if pd.isnull(original_text):\n",
    "        cleaned_text = \"\"\n",
    "    else:\n",
    "        cleaned_text = original_text.lower()\n",
    "        cleaned_text = re.sub(r\"http\\S+|www\\S+\", \"\", cleaned_text)\n",
    "        cleaned_text = re.sub(r\"@\\w+\", \"\", cleaned_text)\n",
    "        cleaned_text = re.sub(r\"#\\w+\", \"\", cleaned_text)\n",
    "        cleaned_text = re.sub(r\"[^\\w\\s]\", \"\", cleaned_text)\n",
    "        cleaned_text = re.sub(r\"\\d+\", \"\", cleaned_text)\n",
    "        cleaned_text = re.sub(r\"\\s+\", \" \", cleaned_text).strip()\n",
    "\n",
    "    # Write the cleaned text to a new file in the same directory\n",
    "    input_dir = os.path.dirname(input_file)\n",
    "    output_path = os.path.join(input_dir, output_file)\n",
    "    \n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        f.write(cleaned_text)\n",
    "\n",
    "    print(f\"Cleaned text written to: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a998a73",
   "metadata": {},
   "source": [
    "# 4.0 Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c9c4e130",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned text written to: chatbot-repo\\cleaned_output.txt\n"
     ]
    }
   ],
   "source": [
    "# renaming the cleaned nlp_data_file to chatbot-repo\n",
    "\n",
    "nlp_data_file = r'chatbot-repo\\nlp_data.txt'\n",
    "\n",
    "clean_text_file(nlp_data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "44903677",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'chatbot-repo\\\\nlp_data.txt'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp_data_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d57cea53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Can we make this quick?  Roxanne Korrine and Andrew Barrett are having an incredibly horrendous public break- up on the quad.  Again.\n",
      "Well, I thought we'd start with pronunciation, if that's okay with you.\n",
      "Not the hacking and gagging and spitting part.  Please.\n",
      "Okay... then how 'bout we try out some French cuisine.  Saturday?  Night?\n",
      "\n",
      "----------------------------------------\n",
      "You're asking me out.  That's so cute. What's your name again?\n",
      "Forget it.\n",
      "\n",
      "----------------------------------------\n",
      "No, no, it's my fault -- we didn't have a proper introduction ---\n",
      "Cameron.\n",
      "The thing is, Cameron -- I'm at the mercy of a particularly hideous breed of loser.  My sister.  I can't date until she does.\n",
      "Seems like she could get a date easy enough...\n",
      "\n",
      "----------------------------------------\n",
      "Why?\n",
      "Unsolved mystery.  She used to be really popular when she started high school, then it was just like she got sick of it or something.\n",
      "That's a shame.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# viewing the data \n",
    "file_path = 'chatbot-repo\\\\nlp_data.txt'\n",
    "with open(file_path, 'r', encoding='utf-8') as file:\n",
    "    for i in range(20):  # Change 20 to however many lines you want to preview\n",
    "        print(file.readline().strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "106c18cf",
   "metadata": {},
   "source": [
    "# 5.0 Creating the BaseLine Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "db7eb526",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading our dataset from a plain text file (one input per line)\n",
    "with open(r\"chatbot-repo\\nlp_data.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "# Create a DataFrame from the lines\n",
    "df = pd.DataFrame({'input': [line.strip() for line in lines if line.strip()]})\n",
    "\n",
    "# Clean the dataset\n",
    "df.dropna(inplace=True)\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Sample a manageable dataset or take all if less than 1000 lines\n",
    "sample_df = df.sample(n=min(1000, len(df)), random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Initialize TF-IDF Vectorizer\n",
    "vectorizer = TfidfVectorizer(analyzer='word', ngram_range=(1, 2), lowercase=True)\n",
    "X_sample = vectorizer.fit_transform(sample_df['input'])\n",
    "\n",
    "# Fit Nearest Neighbors Model\n",
    "nn_model = NearestNeighbors(n_neighbors=1, metric='cosine').fit(X_sample)\n",
    "\n",
    "# Define Chatbot Function\n",
    "def baseline_chatbot(query):\n",
    "    \"\"\"\n",
    "    Multilingual Baseline Chatbot using TF-IDF and Nearest Neighbors.\n",
    "    Supports Swahili and English input.\n",
    "    \"\"\"\n",
    "    query_vec = vectorizer.transform([query])\n",
    "    distance, index = nn_model.kneighbors(query_vec)\n",
    "    return sample_df.loc[index[0][0], 'input']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2c76817d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EN: What?  What is it?\n",
      "SW: But maybe you'd better not.  I've got a witch mad at me, and you might get into trouble.\n"
     ]
    }
   ],
   "source": [
    "# Running - test\n",
    "print(\"EN:\", baseline_chatbot(\"What is AI?\"))\n",
    "print(\"SW:\", baseline_chatbot(\"Habari yako kuhusu teknolojia?\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e12ce4b0",
   "metadata": {},
   "source": [
    "### Coverting the data into a structured format\n",
    "\n",
    "Converting the file from .txt to JSON format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9793f914",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Converted 41554 prompt-response pairs and saved to chatbot-repo\\dialogues.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "input_path = 'chatbot-repo\\\\nlp_data.txt'\n",
    "output_path = 'chatbot-repo\\\\dialogues.json'\n",
    "\n",
    "with open(input_path, 'r', encoding='utf-8') as file:\n",
    "    raw_data = file.read()\n",
    "\n",
    "# Split the text by the separator line\n",
    "blocks = [block.strip() for block in raw_data.split('----------------------------------------') if block.strip()]\n",
    "\n",
    "# Convert each block to a prompt-response pair\n",
    "pairs = []\n",
    "for i in range(0, len(blocks) - 1, 2):  # loop in steps of 2\n",
    "    prompt = \" \".join(blocks[i].splitlines()).strip()\n",
    "    response = \" \".join(blocks[i + 1].splitlines()).strip()\n",
    "    pairs.append({\"prompt\": prompt, \"response\": response})\n",
    "\n",
    "# Save as JSON\n",
    "with open(output_path, 'w', encoding='utf-8') as out_file:\n",
    "    json.dump(pairs, out_file, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"‚úÖ Converted {len(pairs)} prompt-response pairs and saved to {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c6970b5",
   "metadata": {},
   "source": [
    "## Tokenizing the chatbot data\n",
    "\n",
    "Using the following\n",
    "\n",
    "Hugging Face's transformers library,\n",
    "datasets to load the JSON,\n",
    "A tokenizer compatible with the base model (e.g., DialoGPT)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "67be1852",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\hp\\anaconda3\\envs\\learn-env\\lib\\site-packages (4.46.3)\n",
      "Requirement already satisfied: datasets in c:\\users\\hp\\anaconda3\\envs\\learn-env\\lib\\site-packages (3.1.0)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in c:\\users\\hp\\anaconda3\\envs\\learn-env\\lib\\site-packages (from transformers) (0.20.3)\n",
      "Requirement already satisfied: filelock in c:\\users\\hp\\anaconda3\\envs\\learn-env\\lib\\site-packages (from transformers) (3.16.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\hp\\anaconda3\\envs\\learn-env\\lib\\site-packages (from transformers) (1.22.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\hp\\anaconda3\\envs\\learn-env\\lib\\site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\hp\\anaconda3\\envs\\learn-env\\lib\\site-packages (from transformers) (20.4)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in c:\\users\\hp\\anaconda3\\envs\\learn-env\\lib\\site-packages (from transformers) (0.33.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\hp\\anaconda3\\envs\\learn-env\\lib\\site-packages (from transformers) (5.3.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\hp\\anaconda3\\envs\\learn-env\\lib\\site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in c:\\users\\hp\\anaconda3\\envs\\learn-env\\lib\\site-packages (from transformers) (2.32.4)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\hp\\anaconda3\\envs\\learn-env\\lib\\site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: pandas in c:\\users\\hp\\anaconda3\\envs\\learn-env\\lib\\site-packages (from datasets) (1.4.4)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\hp\\anaconda3\\envs\\learn-env\\lib\\site-packages (from datasets) (3.6.3)\n",
      "Requirement already satisfied: xxhash in c:\\users\\hp\\anaconda3\\envs\\learn-env\\lib\\site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in c:\\users\\hp\\anaconda3\\envs\\learn-env\\lib\\site-packages (from datasets) (17.0.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in c:\\users\\hp\\anaconda3\\envs\\learn-env\\lib\\site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\users\\hp\\anaconda3\\envs\\learn-env\\lib\\site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: fsspec[http]<=2024.9.0,>=2023.1.0 in c:\\users\\hp\\anaconda3\\envs\\learn-env\\lib\\site-packages (from datasets) (2024.9.0)\n",
      "Requirement already satisfied: colorama; platform_system == \"Windows\" in c:\\users\\hp\\anaconda3\\envs\\learn-env\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\hp\\anaconda3\\envs\\learn-env\\lib\\site-packages (from packaging>=20.0->transformers) (2.4.7)\n",
      "Requirement already satisfied: six in c:\\users\\hp\\anaconda3\\envs\\learn-env\\lib\\site-packages (from packaging>=20.0->transformers) (1.15.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\hp\\anaconda3\\envs\\learn-env\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.13.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\hp\\anaconda3\\envs\\learn-env\\lib\\site-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\hp\\anaconda3\\envs\\learn-env\\lib\\site-packages (from requests->transformers) (2024.8.30)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\hp\\anaconda3\\envs\\learn-env\\lib\\site-packages (from requests->transformers) (1.25.10)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\hp\\anaconda3\\envs\\learn-env\\lib\\site-packages (from requests->transformers) (3.4.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\hp\\anaconda3\\envs\\learn-env\\lib\\site-packages (from pandas->datasets) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\hp\\anaconda3\\envs\\learn-env\\lib\\site-packages (from pandas->datasets) (2020.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\hp\\anaconda3\\envs\\learn-env\\lib\\site-packages (from aiohttp->datasets) (20.2.0)\n",
      "Requirement already satisfied: async-timeout<4.0,>=3.0 in c:\\users\\hp\\anaconda3\\envs\\learn-env\\lib\\site-packages (from aiohttp->datasets) (3.0.1)\n",
      "Requirement already satisfied: multidict<5.0,>=4.5 in c:\\users\\hp\\anaconda3\\envs\\learn-env\\lib\\site-packages (from aiohttp->datasets) (4.7.5)\n",
      "Requirement already satisfied: chardet<4.0,>=2.0 in c:\\users\\hp\\anaconda3\\envs\\learn-env\\lib\\site-packages (from aiohttp->datasets) (3.0.4)\n",
      "Requirement already satisfied: yarl<1.6.0,>=1.0 in c:\\users\\hp\\anaconda3\\envs\\learn-env\\lib\\site-packages (from aiohttp->datasets) (1.5.1)\n"
     ]
    }
   ],
   "source": [
    "# import the libraries \n",
    "! pip install transformers datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d31fe4e0",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[WinError 127] The specified procedure could not be found. Error loading \"c:\\Users\\hp\\anaconda3\\envs\\learn-env\\lib\\site-packages\\torch\\lib\\shm.dll\" or one of its dependencies.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-39-bcfc7de8df70>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# import the libraries\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mdatasets\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mload_dataset\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\hp\\anaconda3\\envs\\learn-env\\lib\\site-packages\\transformers\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;31m# Check the dependencies satisfy the minimal versions required.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdependency_versions_check\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m from .utils import (\n\u001b[0;32m     28\u001b[0m     \u001b[0mOptionalDependencyNotAvailable\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\hp\\anaconda3\\envs\\learn-env\\lib\\site-packages\\transformers\\dependency_versions_check.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mdependency_versions_table\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdeps\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mversions\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mrequire_version\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrequire_version_core\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\hp\\anaconda3\\envs\\learn-env\\lib\\site-packages\\transformers\\utils\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0m__version__\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mbackbone_utils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mBackboneConfigMixin\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mBackboneMixin\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mchat_template_utils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mDocstringParsingException\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTypeHintParsingException\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mget_json_schema\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mconstants\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mIMAGENET_DEFAULT_MEAN\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mIMAGENET_DEFAULT_STD\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mIMAGENET_STANDARD_MEAN\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mIMAGENET_STANDARD_STD\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m from .doc import (\n",
      "\u001b[1;32mc:\\Users\\hp\\anaconda3\\envs\\learn-env\\lib\\site-packages\\transformers\\utils\\chat_template_utils.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mis_torch_available\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 39\u001b[1;33m     \u001b[1;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     40\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\hp\\anaconda3\\envs\\learn-env\\lib\\site-packages\\torch\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    135\u001b[0m                 \u001b[0merr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mctypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mWinError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlast_error\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    136\u001b[0m                 \u001b[0merr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrerror\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;34mf' Error loading \"{dll}\" or one of its dependencies.'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 137\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    138\u001b[0m             \u001b[1;32melif\u001b[0m \u001b[0mres\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    139\u001b[0m                 \u001b[0mis_loaded\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mOSError\u001b[0m: [WinError 127] The specified procedure could not be found. Error loading \"c:\\Users\\hp\\anaconda3\\envs\\learn-env\\lib\\site-packages\\torch\\lib\\shm.dll\" or one of its dependencies."
     ]
    }
   ],
   "source": [
    "# import the libraries\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a0a8551c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\hp\\anaconda3\\envs\\learn-env\\lib\\site-packages (2.4.1+cpu)\n",
      "Requirement already satisfied: filelock in c:\\users\\hp\\anaconda3\\envs\\learn-env\\lib\\site-packages (from torch) (3.16.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\hp\\anaconda3\\envs\\learn-env\\lib\\site-packages (from torch) (2.11.2)\n",
      "Requirement already satisfied: fsspec in c:\\users\\hp\\anaconda3\\envs\\learn-env\\lib\\site-packages (from torch) (2024.9.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\hp\\anaconda3\\envs\\learn-env\\lib\\site-packages (from torch) (2.5.1)\n",
      "Requirement already satisfied: sympy in c:\\users\\hp\\anaconda3\\envs\\learn-env\\lib\\site-packages (from torch) (1.13.3)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\hp\\anaconda3\\envs\\learn-env\\lib\\site-packages (from torch) (4.13.2)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\hp\\anaconda3\\envs\\learn-env\\lib\\site-packages (from jinja2->torch) (1.1.1)\n",
      "Requirement already satisfied: decorator<5,>=4.3 in c:\\users\\hp\\anaconda3\\envs\\learn-env\\lib\\site-packages (from networkx->torch) (4.4.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\hp\\anaconda3\\envs\\learn-env\\lib\\site-packages (from sympy->torch) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!  pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b79a0cf4",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[WinError 127] The specified procedure could not be found. Error loading \"c:\\Users\\hp\\anaconda3\\envs\\learn-env\\lib\\site-packages\\torch\\lib\\shm.dll\" or one of its dependencies.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-34-beb55f24d5ec>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__version__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\hp\\anaconda3\\envs\\learn-env\\lib\\site-packages\\torch\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    135\u001b[0m                 \u001b[0merr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mctypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mWinError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlast_error\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    136\u001b[0m                 \u001b[0merr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrerror\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;34mf' Error loading \"{dll}\" or one of its dependencies.'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 137\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    138\u001b[0m             \u001b[1;32melif\u001b[0m \u001b[0mres\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    139\u001b[0m                 \u001b[0mis_loaded\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mOSError\u001b[0m: [WinError 127] The specified procedure could not be found. Error loading \"c:\\Users\\hp\\anaconda3\\envs\\learn-env\\lib\\site-packages\\torch\\lib\\shm.dll\" or one of its dependencies."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "20b5ba76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4da388994de94d3a86d65a34ff2c6608",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['prompt', 'response'],\n",
       "        num_rows: 41554\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the JSON File\n",
    "# Load your JSON file\n",
    "data = load_dataset('json', data_files='chatbot-repo/dialogues.json')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "fc3a114b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'AutoTokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-36-b438522dae3b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Tokenizer using DialoGPT:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mmodel_checkpoint\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"microsoft/DialoGPT-medium\"\u001b[0m  \u001b[1;31m# or DialoGPT-small/large\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mtokenizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_checkpoint\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'AutoTokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "# Tokenizer using DialoGPT:\n",
    "model_checkpoint = \"microsoft/DialoGPT-medium\"  # or DialoGPT-small/large\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "cb07cb4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e9520ab",
   "metadata": {},
   "source": [
    "## Concatenate Prompt and Response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "24f42169",
   "metadata": {},
   "outputs": [],
   "source": [
    "## combine the prompt and response into one sequence (with end-of-sequence tokens):\n",
    "def preprocess(example):\n",
    "    # Combine prompt and response with special tokens\n",
    "    combined = example['prompt'] + tokenizer.eos_token + example['response'] + tokenizer.eos_token\n",
    "    return tokenizer(combined, truncation=True, padding=\"max_length\", max_length=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5cfd79a",
   "metadata": {},
   "source": [
    "### Apply tokenization to the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d1e8c57b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0406be1a8c7b4456b7113dd505bab3ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/41554 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "PyTorch needs to be installed to be able to return PyTorch tensors.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-43-41a8fa0a3006>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# apply tokenization to the data set\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mtokenized_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpreprocess\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatched\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mtokenized_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_format\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'torch'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'input_ids'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'attention_mask'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\hp\\anaconda3\\envs\\learn-env\\lib\\site-packages\\datasets\\dataset_dict.py\u001b[0m in \u001b[0;36mset_format\u001b[1;34m(self, type, columns, output_all_columns, **format_kwargs)\u001b[0m\n\u001b[0;32m    596\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_values_type\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    597\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mdataset\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 598\u001b[1;33m             \u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_format\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_all_columns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutput_all_columns\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mformat_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    599\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    600\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mreset_format\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\hp\\anaconda3\\envs\\learn-env\\lib\\site-packages\\datasets\\fingerprint.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    440\u001b[0m             \u001b[1;31m# Call actual function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    441\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 442\u001b[1;33m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    443\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    444\u001b[0m             \u001b[1;31m# Update fingerprint of in-place transforms + update in-place history of transforms\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\hp\\anaconda3\\envs\\learn-env\\lib\\site-packages\\datasets\\arrow_dataset.py\u001b[0m in \u001b[0;36mset_format\u001b[1;34m(self, type, columns, output_all_columns, **format_kwargs)\u001b[0m\n\u001b[0;32m   2511\u001b[0m         \u001b[1;31m# Check that the format_type and format_kwargs are valid and make it possible to have a Formatter\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2512\u001b[0m         \u001b[0mtype\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_format_type_from_alias\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2513\u001b[1;33m         \u001b[0mget_formatter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_info\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mformat_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2514\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2515\u001b[0m         \u001b[1;31m# Check filter column\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\hp\\anaconda3\\envs\\learn-env\\lib\\site-packages\\datasets\\formatting\\__init__.py\u001b[0m in \u001b[0;36mget_formatter\u001b[1;34m(format_type, **format_kwargs)\u001b[0m\n\u001b[0;32m    131\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0m_FORMAT_TYPES\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mformat_type\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mformat_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    132\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mformat_type\u001b[0m \u001b[1;32min\u001b[0m \u001b[0m_FORMAT_TYPES_ALIASES_UNAVAILABLE\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 133\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0m_FORMAT_TYPES_ALIASES_UNAVAILABLE\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mformat_type\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    134\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    135\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"Format type should be one of {list(_FORMAT_TYPES.keys())}, but got '{format_type}'\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "    \u001b[1;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "\u001b[1;32m<ipython-input-32-41a8fa0a3006>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# apply tokenization to the data set\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mtokenized_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpreprocess\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatched\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mtokenized_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_format\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'torch'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'input_ids'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'attention_mask'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\hp\\anaconda3\\envs\\learn-env\\lib\\site-packages\\datasets\\dataset_dict.py\u001b[0m in \u001b[0;36mset_format\u001b[1;34m(self, type, columns, output_all_columns, **format_kwargs)\u001b[0m\n\u001b[0;32m    596\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_values_type\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    597\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mdataset\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 598\u001b[1;33m             \u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_format\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_all_columns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutput_all_columns\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mformat_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    599\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    600\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mreset_format\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\hp\\anaconda3\\envs\\learn-env\\lib\\site-packages\\datasets\\fingerprint.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    440\u001b[0m             \u001b[1;31m# Call actual function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    441\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 442\u001b[1;33m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    443\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    444\u001b[0m             \u001b[1;31m# Update fingerprint of in-place transforms + update in-place history of transforms\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\hp\\anaconda3\\envs\\learn-env\\lib\\site-packages\\datasets\\arrow_dataset.py\u001b[0m in \u001b[0;36mset_format\u001b[1;34m(self, type, columns, output_all_columns, **format_kwargs)\u001b[0m\n\u001b[0;32m   2511\u001b[0m         \u001b[1;31m# Check that the format_type and format_kwargs are valid and make it possible to have a Formatter\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2512\u001b[0m         \u001b[0mtype\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_format_type_from_alias\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2513\u001b[1;33m         \u001b[0mget_formatter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_info\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mformat_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2514\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2515\u001b[0m         \u001b[1;31m# Check filter column\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\hp\\anaconda3\\envs\\learn-env\\lib\\site-packages\\datasets\\formatting\\__init__.py\u001b[0m in \u001b[0;36mget_formatter\u001b[1;34m(format_type, **format_kwargs)\u001b[0m\n\u001b[0;32m    131\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0m_FORMAT_TYPES\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mformat_type\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mformat_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    132\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mformat_type\u001b[0m \u001b[1;32min\u001b[0m \u001b[0m_FORMAT_TYPES_ALIASES_UNAVAILABLE\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 133\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0m_FORMAT_TYPES_ALIASES_UNAVAILABLE\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mformat_type\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    134\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    135\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"Format type should be one of {list(_FORMAT_TYPES.keys())}, but got '{format_type}'\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "    \u001b[1;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "\u001b[1;32m<ipython-input-26-41a8fa0a3006>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# apply tokenization to the data set\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mtokenized_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpreprocess\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatched\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mtokenized_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_format\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'torch'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'input_ids'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'attention_mask'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\hp\\anaconda3\\envs\\learn-env\\lib\\site-packages\\datasets\\dataset_dict.py\u001b[0m in \u001b[0;36mset_format\u001b[1;34m(self, type, columns, output_all_columns, **format_kwargs)\u001b[0m\n\u001b[0;32m    596\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_values_type\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    597\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mdataset\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 598\u001b[1;33m             \u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_format\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_all_columns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutput_all_columns\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mformat_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    599\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    600\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mreset_format\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\hp\\anaconda3\\envs\\learn-env\\lib\\site-packages\\datasets\\fingerprint.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    440\u001b[0m             \u001b[1;31m# Call actual function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    441\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 442\u001b[1;33m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    443\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    444\u001b[0m             \u001b[1;31m# Update fingerprint of in-place transforms + update in-place history of transforms\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\hp\\anaconda3\\envs\\learn-env\\lib\\site-packages\\datasets\\arrow_dataset.py\u001b[0m in \u001b[0;36mset_format\u001b[1;34m(self, type, columns, output_all_columns, **format_kwargs)\u001b[0m\n\u001b[0;32m   2511\u001b[0m         \u001b[1;31m# Check that the format_type and format_kwargs are valid and make it possible to have a Formatter\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2512\u001b[0m         \u001b[0mtype\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_format_type_from_alias\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2513\u001b[1;33m         \u001b[0mget_formatter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_info\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mformat_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2514\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2515\u001b[0m         \u001b[1;31m# Check filter column\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\hp\\anaconda3\\envs\\learn-env\\lib\\site-packages\\datasets\\formatting\\__init__.py\u001b[0m in \u001b[0;36mget_formatter\u001b[1;34m(format_type, **format_kwargs)\u001b[0m\n\u001b[0;32m    131\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0m_FORMAT_TYPES\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mformat_type\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mformat_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    132\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mformat_type\u001b[0m \u001b[1;32min\u001b[0m \u001b[0m_FORMAT_TYPES_ALIASES_UNAVAILABLE\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 133\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0m_FORMAT_TYPES_ALIASES_UNAVAILABLE\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mformat_type\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    134\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    135\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"Format type should be one of {list(_FORMAT_TYPES.keys())}, but got '{format_type}'\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: PyTorch needs to be installed to be able to return PyTorch tensors."
     ]
    }
   ],
   "source": [
    "# apply tokenization to the data set\n",
    "tokenized_data = data.map(preprocess, batched=False)\n",
    "tokenized_data.set_format(type='torch', columns=['input_ids', 'attention_mask'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bb4cd99",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e98728c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f8360236",
   "metadata": {},
   "source": [
    "# Next Steps..\n",
    "# - Preprocess the dialogues\n",
    "# - Tokenize\n",
    "# - Pair conversations into input-output for chatbot modeling\n",
    "# - Use with Seq2Seq models: Like an encoder-decoder for chatbot training\n",
    "# - Store as CSV: Two columns: input_line, target_line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15dc9230",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
