{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "31a3b6e8",
   "metadata": {},
   "source": [
    "## Capstone Project Title: Developmet of a Natural Language Multi-lingual Chatbot for the Kenyan Market"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dbc33cf",
   "metadata": {},
   "source": [
    "## Team Members \n",
    "\n",
    "1. Jessica Mutiso\n",
    "2. Brian Waweru\n",
    "3. Pamela Godia\n",
    "4. Hellen Mwaniki"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d2cdc89",
   "metadata": {},
   "source": [
    "## 1. Project Overview \n",
    "\n",
    "This project aims to develop a natural language chatbot capable of generating human-like responses and understanding informal customer feedback expressed in English, Kenyan Swahili and Sheng. Designed for a startup expanding into the Kenyan market, the chatbot will help the company engage users more naturally and analyze feedback from social platforms and online conversations. By training on locally relevant dialogue data  including YouTube comments and Kenyan media the system will capture the linguistic and cultural nuances often missed by standard models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0315764",
   "metadata": {},
   "source": [
    "## 1.1 Problem Statement\n",
    "Startups entering new markets often struggle to understand customer feedback when it's expressed in local dialects or informal language. In Kenya, much of this communication occurs in Swahili and Sheng, which combine local slang, English, and Swahili in a fluid, often unstructured manner. Existing chatbot systems trained on formal English fail to grasp the tone, intent, or meaning behind such messages. This project aims to fill that gap by building a chatbot trained specifically on real-world Kenyan conversations to interpret and respond to customer queries and feedback with local context and relevance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f28d3e7b",
   "metadata": {},
   "source": [
    "## 1.2 Objectives\n",
    "\n",
    "- Collect and preprocess Kenyan user dialogue from YouTube, social media, and local content featuring Swahili and Sheng\n",
    "\n",
    "- Fine-tune the chatbot with foundational data for conversational structure, while emphasizing local language patterns\n",
    "\n",
    "- Build a sequence-to-sequence model  capable of handling informal, code-switched dialogue\n",
    "\n",
    "- Evaluate the chatbot‚Äôs performance with emphasis on contextual relevance and local understanding\n",
    "\n",
    "- Present a working prototype that simulates real customer feedback scenarios "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03fbf2f0",
   "metadata": {},
   "source": [
    "## 2.0 Data Understanding "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bd4b9de",
   "metadata": {},
   "source": [
    "## 2.1 In this project we sourced data from 2 sources\n",
    "      the cornell  movie corpus\n",
    "      youtube comments from local Kenyan content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8625e758",
   "metadata": {},
   "source": [
    "## 2.2 Loading the datasets\n",
    "movie_lines.txt and movie_conversations.txt\n",
    "the dataset encoding format is  encoding='ISO-8859-1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7feb0a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing the necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import ast\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import string\n",
    "\n",
    "# Read the lines\n",
    "lines = pd.read_csv('movie_lines.txt', sep=r' \\+\\+\\+\\$\\+\\+\\+ ', engine='python', header=None,\n",
    "                    names=['lineID', 'characterID', 'movieID', 'character', 'text'],  encoding='ISO-8859-1')\n",
    "\n",
    "# Read the conversations\n",
    "conversations = pd.read_csv('movie_conversations.txt', sep=r' \\+\\+\\+\\$\\+\\+\\+ ', engine='python', header=None,\n",
    "                            names=['char1', 'char2', 'movieID', 'utteranceIDs'],  encoding='ISO-8859-1')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bc97db8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>video_id</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>qlZM3McwO1Q</td>\n",
       "      <td>What an incredible victory. I agree the Kenyan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>qlZM3McwO1Q</td>\n",
       "      <td>‚ù§</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>qlZM3McwO1Q</td>\n",
       "      <td>‚ÄúClaudia is an amazonian goddess with a beauti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>qlZM3McwO1Q</td>\n",
       "      <td>Proud of my motherland Kenya ‚ù§‚ù§‚ù§and Africa.at ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>qlZM3McwO1Q</td>\n",
       "      <td>Damn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25624</th>\n",
       "      <td>q4sWUJxjc4g</td>\n",
       "      <td>Welcome home‚ù§Ô∏è</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25625</th>\n",
       "      <td>q4sWUJxjc4g</td>\n",
       "      <td>Uweeeeeh... All the best, lakini...üòí</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25626</th>\n",
       "      <td>q4sWUJxjc4g</td>\n",
       "      <td>ü•µwoii finally our girl Lynn is here to ask goo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25627</th>\n",
       "      <td>q4sWUJxjc4g</td>\n",
       "      <td>As usual  ü¶ª</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25628</th>\n",
       "      <td>q4sWUJxjc4g</td>\n",
       "      <td>Hello Lynn family... and first to be here</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>25629 rows √ó 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          video_id                                               text\n",
       "0      qlZM3McwO1Q  What an incredible victory. I agree the Kenyan...\n",
       "1      qlZM3McwO1Q                                                  ‚ù§\n",
       "2      qlZM3McwO1Q  ‚ÄúClaudia is an amazonian goddess with a beauti...\n",
       "3      qlZM3McwO1Q  Proud of my motherland Kenya ‚ù§‚ù§‚ù§and Africa.at ...\n",
       "4      qlZM3McwO1Q                                               Damn\n",
       "...            ...                                                ...\n",
       "25624  q4sWUJxjc4g                                     Welcome home‚ù§Ô∏è\n",
       "25625  q4sWUJxjc4g               Uweeeeeh... All the best, lakini...üòí\n",
       "25626  q4sWUJxjc4g  ü•µwoii finally our girl Lynn is here to ask goo...\n",
       "25627  q4sWUJxjc4g                                        As usual  ü¶ª\n",
       "25628  q4sWUJxjc4g          Hello Lynn family... and first to be here\n",
       "\n",
       "[25629 rows x 2 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load scraped YouTube comments\n",
    "yt_comments_one= pd.read_csv('youtube_comments.csv')\n",
    "\n",
    "# Rename if needed\n",
    "yt_comments_one.rename(columns={'comment': 'text'}, inplace=True)\n",
    "\n",
    "#preview the dataset\n",
    "yt_comments_one.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0edc7d35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Top Comment</th>\n",
       "      <th>Reply</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Apple missed the boat on AI OR... Apple is doi...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Who added the background music to the video it...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>16:26  FEMI KUTI !!! RAAHHH !!!</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The greatest AI scam in history, is AI.</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>if I only knew Siri is a mess, I would bought ...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2848</th>\n",
       "      <td>Still waiting for Apple Intelligence to be mor...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2849</th>\n",
       "      <td>Today's apple is full of failures.\\r\\nI'm swit...</td>\n",
       "      <td>Yes! It was once a superior design product but...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2850</th>\n",
       "      <td>Finally</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2851</th>\n",
       "      <td>first comment √∞≈∏≈Ω‚Ä∞</td>\n",
       "      <td>Wow, . . . You Were First To Attain OMEGA Stat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2852</th>\n",
       "      <td>Bombaclat</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2853 rows √ó 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Top Comment  \\\n",
       "0     Apple missed the boat on AI OR... Apple is doi...   \n",
       "1     Who added the background music to the video it...   \n",
       "2                       16:26  FEMI KUTI !!! RAAHHH !!!   \n",
       "3               The greatest AI scam in history, is AI.   \n",
       "4     if I only knew Siri is a mess, I would bought ...   \n",
       "...                                                 ...   \n",
       "2848  Still waiting for Apple Intelligence to be mor...   \n",
       "2849  Today's apple is full of failures.\\r\\nI'm swit...   \n",
       "2850                                            Finally   \n",
       "2851                                 first comment √∞≈∏≈Ω‚Ä∞   \n",
       "2852                                          Bombaclat   \n",
       "\n",
       "                                                  Reply  \n",
       "0                                                   NaN  \n",
       "1                                                   NaN  \n",
       "2                                                   NaN  \n",
       "3                                                   NaN  \n",
       "4                                                   NaN  \n",
       "...                                                 ...  \n",
       "2848                                                NaN  \n",
       "2849  Yes! It was once a superior design product but...  \n",
       "2850                                                NaN  \n",
       "2851  Wow, . . . You Were First To Attain OMEGA Stat...  \n",
       "2852                                                NaN  \n",
       "\n",
       "[2853 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load scraped YouTube comments\n",
    "yt_comments_two= pd.read_csv('data-from-youtube.csv')\n",
    "\n",
    "# Rename if needed\n",
    "yt_comments_two.rename(columns={'comment': 'text'}, inplace=True)\n",
    "\n",
    "#preview the dataset\n",
    "yt_comments_two"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3dffbd38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 25629 entries, 0 to 25628\n",
      "Data columns (total 2 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   video_id  25629 non-null  object\n",
      " 1   text      25628 non-null  object\n",
      "dtypes: object(2)\n",
      "memory usage: 400.6+ KB\n"
     ]
    }
   ],
   "source": [
    "yt_comments_one.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "48b90157",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2853 entries, 0 to 2852\n",
      "Data columns (total 2 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   Top Comment  2850 non-null   object\n",
      " 1   Reply        467 non-null    object\n",
      "dtypes: object(2)\n",
      "memory usage: 44.7+ KB\n"
     ]
    }
   ],
   "source": [
    "yt_comments_two.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "85764697",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 304713 entries, 0 to 304712\n",
      "Data columns (total 5 columns):\n",
      " #   Column       Non-Null Count   Dtype \n",
      "---  ------       --------------   ----- \n",
      " 0   lineID       304713 non-null  object\n",
      " 1   characterID  304713 non-null  object\n",
      " 2   movieID      304713 non-null  object\n",
      " 3   character    304670 non-null  object\n",
      " 4   text         304446 non-null  object\n",
      "dtypes: object(5)\n",
      "memory usage: 11.6+ MB\n"
     ]
    }
   ],
   "source": [
    "lines.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5cc1b526",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 83097 entries, 0 to 83096\n",
      "Data columns (total 4 columns):\n",
      " #   Column        Non-Null Count  Dtype \n",
      "---  ------        --------------  ----- \n",
      " 0   char1         83097 non-null  object\n",
      " 1   char2         83097 non-null  object\n",
      " 2   movieID       83097 non-null  object\n",
      " 3   utteranceIDs  83097 non-null  object\n",
      "dtypes: object(4)\n",
      "memory usage: 2.5+ MB\n"
     ]
    }
   ],
   "source": [
    "conversations.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f39063a",
   "metadata": {},
   "source": [
    "Due to the large size of the dataset in this case sampling will be used to make the dataset that is to be merged managable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cdcba71e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  char1 char2 movieID              utteranceIDs\n",
      "0    u0    u2      m0  [L194, L195, L196, L197]\n",
      "1    u0    u2      m0              [L198, L199]\n",
      "2    u0    u2      m0  [L200, L201, L202, L203]\n",
      "3    u0    u2      m0        [L204, L205, L206]\n",
      "4    u0    u2      m0              [L207, L208]\n"
     ]
    }
   ],
   "source": [
    "# Convert the string representation of lists to actual Python lists\n",
    "conversations['utteranceIDs'] = conversations['utteranceIDs'].apply(ast.literal_eval)\n",
    "\n",
    "# Preview\n",
    "print(conversations.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed291e47",
   "metadata": {},
   "source": [
    "Sample 10,000 lines from the movie_lines dataset and 5000 lines from yt_comments_one and 2500 from yt_comments_two"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "599761e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      video_id                                               text\n",
      "0  _b6D5wMzKZQ  God is hearing your cries.  You will overcome....\n",
      "1  q4sWUJxjc4g           I feel for Sarah . This is so annoying‚Ä¶‚Ä¶\n",
      "2  _b6D5wMzKZQ  I think he suffocated the child till he had no...\n",
      "3  _b6D5wMzKZQ  It is so emotional üò≠üò≠üò≠üò≠..may the lord hear her...\n",
      "4  _b6D5wMzKZQ                  This is too painful to be serious\n"
     ]
    }
   ],
   "source": [
    "# Shuffle the data and select 10,000 lines\n",
    "sampled_yt1 = yt_comments_one.sample(n=5000, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Preview\n",
    "print(sampled_yt1.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bbef7c5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                         Top Comment  \\\n",
      "0  The only thing Apple has been successful at is...   \n",
      "1  looks like its tim cooks inability to manage A...   \n",
      "2  The irony of using generative AI on your visua...   \n",
      "3  I set the charging to 80% and in some cases, i...   \n",
      "4  No offence, but this is clickbait shite. All s...   \n",
      "\n",
      "                                               Reply  \n",
      "0                     that is why they are brilliant  \n",
      "1                                                NaN  \n",
      "2  The #1 easiest way to lose respect as a creato...  \n",
      "3                                                NaN  \n",
      "4                                                NaN  \n"
     ]
    }
   ],
   "source": [
    "# Shuffle the data and select 10,000 lines\n",
    "sampled_yt2= yt_comments_two.sample(n=2500, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Preview\n",
    "print(sampled_yt2.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ed327094",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    lineID characterID movieID character  \\\n",
      "0  L350675       u1846    m121     NICKY   \n",
      "1   L69531       u3916    m259       MAX   \n",
      "2  L101238       u4195    m280  CINNABAR   \n",
      "3  L359643       u6415    m427     SIDRA   \n",
      "4  L155329       u4758    m316      REED   \n",
      "\n",
      "                                                text  \n",
      "0                              My mom wasn't a goat?  \n",
      "1  I must protect my interests, Ms. Kyle.  And In...  \n",
      "2  You said bad things hurt places.  So maybe goo...  \n",
      "3                            She hates all freshmen.  \n",
      "4                             What are you gonna do?  \n"
     ]
    }
   ],
   "source": [
    "# Shuffle the data and select 10,000 lines\n",
    "sampled_lines = lines.sample(n=10000, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Preview\n",
    "print(sampled_lines.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1bddda3",
   "metadata": {},
   "source": [
    "Clean and Normalize Text\n",
    "\n",
    "Preprocess Text Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3e9f6ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    if pd.isnull(text):\n",
    "        return \"\"\n",
    "    \n",
    "    text = text.lower()                          # Lowercase all text\n",
    "    text = re.sub(r\"http\\S+|www\\S+\", \"\", text)   # Remove URLs\n",
    "    text = re.sub(r\"@\\w+\", \"\", text)             # Remove mentions\n",
    "    text = re.sub(r\"#\\w+\", \"\", text)             # Remove hashtags\n",
    "    text = re.sub(r\"[^\\w\\s]\", \"\", text)          # Remove punctuation\n",
    "    text = re.sub(r\"\\d+\", \"\", text)              # Remove digits\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()     # Remove extra whitespace\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b6112981",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_lines['clean_text'] = sampled_lines['text'].apply(clean_text)\n",
    "sampled_yt1['clean_text'] = sampled_yt1['text'].apply(clean_text)\n",
    "sampled_yt2['clean_text'] = sampled_yt2['Top Comment'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1af3a9c3",
   "metadata": {},
   "source": [
    "Link Lines to Conversations\n",
    "We want to expand the utteranceIDs list so we can join each ID to its actual text from movie_lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9b211837",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   conversationID lineID\n",
      "0               0   L194\n",
      "1               0   L195\n",
      "2               0   L196\n",
      "3               0   L197\n",
      "4               1   L198\n"
     ]
    }
   ],
   "source": [
    "#flatten conversation utterance IDs into pairs of (conversationID, lineID)\n",
    "conversation_data = []\n",
    "\n",
    "for idx, row in conversations.iterrows():\n",
    "    for line_id in row['utteranceIDs']:\n",
    "        conversation_data.append({\n",
    "            'conversationID': idx,  # Assign index as a unique conversation ID\n",
    "            'lineID': line_id\n",
    "        })\n",
    "\n",
    "#convert to DataFrame\n",
    "conv_expanded = pd.DataFrame(conversation_data)\n",
    "\n",
    "#preview\n",
    "print(conv_expanded.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d9d9d7d",
   "metadata": {},
   "source": [
    "Merge Expanded Conversations with Sampled Lines\n",
    "\n",
    "This lets us know which lines from the conversations are in our 10,000-line sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1e2e7855",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   conversationID lineID characterID movieID character  \\\n",
      "0               5   L275          u0      m0    BIANCA   \n",
      "1              19   L862          u0      m0    BIANCA   \n",
      "2              37   L759          u4      m0      JOEY   \n",
      "3              44   L543          u5      m0       KAT   \n",
      "4              48   L898          u0      m0    BIANCA   \n",
      "\n",
      "                                            text  \\\n",
      "0                                 Forget French.   \n",
      "1                    do you listen to this crap?   \n",
      "2  Listen, I want to talk to you about the prom.   \n",
      "3                                 Can we go now?   \n",
      "4                              But you hate Joey   \n",
      "\n",
      "                                    clean_text  \n",
      "0                                forget french  \n",
      "1                   do you listen to this crap  \n",
      "2  listen i want to talk to you about the prom  \n",
      "3                                can we go now  \n",
      "4                            but you hate joey  \n"
     ]
    }
   ],
   "source": [
    "#Merge sampled lines with conversation data\n",
    "merged = pd.merge(conv_expanded, sampled_lines, on='lineID')\n",
    "\n",
    "#Preview merged data\n",
    "print(merged.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5787df1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 10000 entries, 0 to 9999\n",
      "Data columns (total 7 columns):\n",
      " #   Column          Non-Null Count  Dtype \n",
      "---  ------          --------------  ----- \n",
      " 0   conversationID  10000 non-null  int64 \n",
      " 1   lineID          10000 non-null  object\n",
      " 2   characterID     10000 non-null  object\n",
      " 3   movieID         10000 non-null  object\n",
      " 4   character       9999 non-null   object\n",
      " 5   text            9992 non-null   object\n",
      " 6   clean_text      10000 non-null  object\n",
      "dtypes: int64(1), object(6)\n",
      "memory usage: 625.0+ KB\n"
     ]
    }
   ],
   "source": [
    "merged.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "528c4032",
   "metadata": {},
   "source": [
    "merge all the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52e102a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            text response   source Top Comment\n",
      "0                                 Forget French.     None  cornell         NaN\n",
      "1                    do you listen to this crap?     None  cornell         NaN\n",
      "2  Listen, I want to talk to you about the prom.     None  cornell         NaN\n",
      "3                                 Can we go now?     None  cornell         NaN\n",
      "4                              But you hate Joey     None  cornell         NaN\n",
      "cornell        10000\n",
      "youtube_one     5000\n",
      "youtube_two     2500\n",
      "Name: source, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "cornell = merged[['text']].copy()\n",
    "cornell['response'] = None \n",
    "cornell['source'] = 'cornell'\n",
    "\n",
    "\n",
    "# Select and rename columns for consistency\n",
    "yt1 =sampled_yt1[['text']].copy()\n",
    "yt1['response'] = None\n",
    "yt1['source'] = 'youtube_one'\n",
    "\n",
    "yt2 = sampled_yt2[['Top Comment']].copy()\n",
    "yt2['response'] = None\n",
    "yt2['source'] = 'youtube_two'\n",
    "\n",
    "final_chatbot_data = pd.concat([cornell, yt1, yt2], ignore_index=True)\n",
    "\n",
    "print(final_chatbot_data.head())\n",
    "print(final_chatbot_data['source'].value_counts())\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
